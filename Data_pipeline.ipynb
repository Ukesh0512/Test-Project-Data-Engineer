{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "c0d3ed70-4f2d-4fbc-91ca-e437745a1720",
      "cell_type": "code",
      "source": "#Task\n#Test Project: Data Engineer",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "4621cfbf-b2bb-4b64-a514-8c69568515d2",
      "cell_type": "code",
      "source": "#Step 1: Data Pipeline Creation\n#To create a data pipeline, we'll use Python with the pandas library for data manipulation and sqlalchemy for database interactions.\n\n#Extract\n#First, we'll extract the data from the CSV files. Let's assume we have three CSV files: social_media.csv, email_campaigns.csv, and web_traffic.csv.\nimport pandas as pd\n\n# Extract data from CSV files\nsocial_media_df = pd.read_csv('social_media.csv')\nemail_campaigns_df = pd.read_csv('email_campaigns.csv')\nweb_traffic_df = pd.read_csv('web_traffic.csv')\n\n#Transform\n#Next, we'll transform the data by cleaning up missing or incorrect values, normalizing the data, and aggregating some data.\n# Clean up missing or incorrect values\nsocial_media_df.fillna(0, inplace=True)\nemail_campaigns_df.fillna(0, inplace=True)\nweb_traffic_df.fillna(0, inplace=True)\n\n# Normalize date formats\nsocial_media_df['date'] = pd.to_datetime(social_media_df['date'])\nemail_campaigns_df['date'] = pd.to_datetime(email_campaigns_df['date'])\nweb_traffic_df['date'] = pd.to_datetime(web_traffic_df['date'])\n\n# Aggregate daily traffic metrics by week\nweb_traffic_df['week'] = web_traffic_df['date'].dt.week\nweb_traffic_df_weekly = web_traffic_df.groupby('week')['sessions', 'bounce_rate', 'conversion_rate'].sum()\n\n#Load\n#Finally, we'll load the cleaned and transformed data into a database. For this example, we'll use a SQLite database.\nimport sqlalchemy as db\n\n# Create a database connection\nengine = db.create_engine('sqlite:///marketing_data.db')\nconn = engine.connect()\n\n# Load data into the database\nsocial_media_df.to_sql('social_media', conn, if_exists='replace', index=False)\nemail_campaigns_df.to_sql('email_campaigns', conn, if_exists='replace', index=False)\nweb_traffic_df_weekly.to_sql('web_traffic_weekly', conn, if_exists='replace', index=False)\n\n## Close the database connection\nconn.close()",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "2d8c2bd8-6fa1-442a-933e-d5d3b3b26b95",
      "cell_type": "code",
      "source": "#Step 2: Database Design\n#For the database design, we'll create three tables: social_media, email_campaigns, and web_traffic_weekly. Here's the schema:\nCREATE TABLE social_media (\n    id INTEGER PRIMARY KEY,\n    date DATE,\n    platform TEXT,\n    engagement INTEGER\n);\n\nCREATE TABLE email_campaigns (\n    id INTEGER PRIMARY KEY,\n    date DATE,\n    campaign_name TEXT,\n    open_rate REAL,\n    click_through_rate REAL\n);\n\nCREATE TABLE web_traffic_weekly (\n    week INTEGER PRIMARY KEY,\n    sessions INTEGER,\n    bounce_rate REAL,\n    conversion_rate REAL\n);",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "51c3c750-671d-44d3-906b-577aa7ba95af",
      "cell_type": "code",
      "source": "#Step 3: Data Querying\n#Here are some sample queries to demonstrate how the data can be retrieved and analyzed:\n#-- Fetch weekly trends in social media engagement\nSELECT week, SUM(engagement) AS total_engagement\nFROM social_media\nGROUP BY week\nORDER BY week;\n\n#-- Retrieve the top 3 email campaigns by click-through rate\nSELECT campaign_name, click_through_rate\nFROM email_campaigns\nORDER BY click_through_rate DESC\nLIMIT 3;\n\n#-- Summarize web traffic by traffic source\nSELECT traffic_source, SUM(sessions) AS total_sessions\nFROM web_traffic_weekly\nGROUP BY traffic_source;",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "f4b0b7a4-ebf0-416c-be96-81d34ebbd9b0",
      "cell_type": "code",
      "source": "#Step 4: Bonus (Optional)\n#For the bonus task, we can set up a simple job scheduler using schedule library to automate the pipeline.\n\n#Automation\n#Create a new Python script, scheduler.py, to schedule the pipeline to run daily at midnight:\nimport schedule\nimport time\n\ndef run_pipeline():\n    # Run the data pipeline script\n    exec(open('data_pipeline.py').read())\n\nschedule.every(1).day.at(\"00:00\").do(run_pipeline)  # Run the pipeline daily at midnight\n\nwhile True:\n    schedule.run_pending()\n    time.sleep(1)\n#Alternatively, we can use Airflow or Cron for job scheduling.\n\n#Data Visualization\n#For data visualization, we can use a tool like Tableau or Google Data Studio to create a dashboard. Here's an example using Google Data Studio:\n\n#Create a new Python script, dashboard.py, to create a dashboard:\nimport pandas as pd\nfrom google.oauth2 import service_account\nfrom googleapiclient.discovery import build\n\n# Create a Google Data Studio dashboard\ncreds = service_account.Credentials.from_service_account_file('credentials.json')\nservice = build('datastudio', 'v1', credentials=creds)\n\n# Create a new dashboard\ndashboard = service.projects().dashboards().create(\n    body={\n        'dashboard': {\n            'title': 'Marketing Data Dashboard'\n        }\n    }\n).execute()\n\n# Add a chart to the dashboard\nchart = service.projects().dashboards().charts().create(\n    dashboardId=dashboard['dashboard']['id'],\n    body={\n        'chart': {\n            'title': 'Social Media Engagement',\n            'type': 'LINE_CHART',\n            'data': {\n                'dataSourceId': 'social_media',\n                'dimensions': ['date'],\n                'metrics': ['engagement']\n            }\n        }\n    }\n).execute()\n# Add more charts and widgets to the dashboard as needed\n\n#Create a credentials.json file with our Google Data Studio credentials:\n{\n  \"type\": \"service_account\",\n  \"project_id\": \"your-project-id\",\n  \"private_key_id\": \"your-private-key-id\",\n  \"private_key\": \"your-private-key\",\n  \"client_email\": \"your-client-email\",\n  \"client_id\": \"your-client-id\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://oauth2.googleapis.com/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/your-service-account-email\"\n}\n#Run the dashboard.py script to create the dashboard. You can then access the dashboard through the Google Data Studio interface.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}